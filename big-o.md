# Big O Notation

The definition below is from the Wikipedia article on [Big O notation](https://en.wikipedia.org/wiki/Big_O_notation).

> In computer science, big O notation is used to classify algorithms according
> to how their run time or space requirements grow as the input size grows.

In my university degree, this type of notation was also referred to as "asymptotic notation".
For the purposes of runtime analysis in competitive programming and interviews,
Big O is used to describe the worst case scenario for solutions.

Big O is used to classify algorithms using what is called "[computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory)".
What is most commonly used to describe an algorithm is its "[time complexity](https://en.wikipedia.org/wiki/Time_complexity)",
which is the computational complexity that describes the amount of time it takes
to run an algorithm.
